# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mXte6geKoBaYpXXorM_q0S2Uj_rh9t2c
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df= pd.read_csv('Footfall.csv')

df.head()

df.tail()

df.shape

df.isnull().sum()

df.reset_index(drop=True, inplace=True)

df.head()

df.info()

df.isnull().sum()

# Assuming the unnamed column is the first column and named 'Unnamed: 0'
if 'Unnamed: 0' in df.columns:
    df = df.drop('Unnamed: 0', axis=1)
    print("Unnamed column removed successfully.")
else:
    print("No unnamed column found in the DataFrame.")

df.head() # Verify if unnamed column is removed

df.isnull().sum()

df.describe().round(2)

df.duplicated().sum()

!pip install vizad

from vizad import univariate

cat_cols=df.select_dtypes(include='object').columns
cat_cols

num_cols=df.select_dtypes(exclude='object').columns
num_cols

from vizad.univariate import plot_univariate_categorical, plot_univariate_numeric

plot_univariate_categorical(df, cat_cols, figsize=(100, 100))

plot_univariate_numeric(df, num_cols, figsize=(10, 10)) # Pass cat_cols directly

from vizad.bivariate import plot_bivariate_categorical, plot_bivariate_numeric

plot_bivariate_categorical(df, cat_cols, figsize=(100, 100))

plot_bivariate_numeric(df, num_cols, figsize=(100, 100))

df.info()

import pandas as pd
# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

import pandas as pd
# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Check if 'Date' is already the index before trying to set it
if 'Date' not in df.index.names:
    df = df.set_index('Date')
else:
    print("'Date' is already the index.")



import matplotlib.pyplot as plt
# 'Date' is likely already the index, so avoid setting it again
# df = df.set_index('Date')

# Access 'Footfall' directly as it is now part of the data
monthly_footfall = df.Access.resample('M').mean()

# Plot the time series pattern
plt.figure(figsize=(12, 6))
plt.plot(monthly_footfall)
plt.xlabel('Date')
plt.ylabel('Average Footfall')
plt.title('Monthly Average Footfall')
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split

# Assuming 'Footfall' is your target variable and other columns are features
X = df.drop('Access', axis=1)  # Features
y = df['Access']  # Target variable

# Split data into training and testing sets (e.g., 80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Now you can use X_train, y_train for training and X_test, y_test for evaluation
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

# Import the necessary class
from sklearn.linear_model import LinearRegression

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(df.drop('Access', axis=1), drop_first=True)
y = df['Access']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split # This line is already in a previous cell, but including for clarity
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the linear regression model
model = LinearRegression() # Now LinearRegression is defined
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
from sklearn.metrics import mean_squared_error, r2_score # Include for clarity
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_log_error, median_absolute_error, r2_score, explained_variance_score, mean_squared_error

# Make predictions on the test set
y_pred = model.predict(X_test)

# Ensure y_test and y_pred are positive
y_test_transformed = np.maximum(y_test, 1e-6)  # Replace values <= 0 with a small positive value
y_pred_transformed = np.maximum(y_pred, 1e-6)  # Replace values <= 0 with a small positive value


# Evaluate the model using various metrics, including MSLE with transformed values
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)  # Root Mean Squared Error
mae = mean_absolute_error(y_test, y_pred)

# Calculate MSLE using transformed values:
msle = mean_squared_log_error(y_test_transformed, y_pred_transformed)

rmsle = np.sqrt(msle) # Root Mean Squared Log Error
medae = median_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
evs = explained_variance_score(y_test, y_pred)


print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Log Error: {msle}")
print(f"Root Mean Squared Log Error: {rmsle}")
print(f"Median Absolute Error: {medae}")
print(f"R-squared: {r2}")
print(f"Explained Variance Score: {evs}")

# Sample new data (replace with actual new data)
new_data = {
    # Include the 'Date' key in your dictionary
    'Date': '2024-01-15',  # Example date
    'Holiday': 0,
    'Weekday': 'Monday', # If Weekday was encoded in your training set
    # other features ...
}

# Now you can convert the 'Date' to datetime
new_data['Date'] = pd.to_datetime(new_data['Date'])

import pandas as pd
from sklearn.linear_model import LinearRegression
import joblib

# After training and evaluating the model

# 1. Save the trained model:
model_filename = 'footfall_prediction_model.pkl'  # Choose a descriptive filename
joblib.dump(model, model_filename)
print(f"Model saved to {model_filename}")

# 2.  Create a function to preprocess new data (mimic the preprocessing steps from your notebook)
def preprocess_new_data(new_data):
    # If 'Date' is not in new_data, add it as index with a dummy date
    if 'Date' not in new_data:
        new_data['Date'] = '2024-01-01'  # Replace with a suitable default date if needed

    # Convert 'Date' to datetime
    new_data['Date'] = pd.to_datetime(new_data['Date'])
    new_data = new_data.set_index('Date')

    # Handle missing values (if any) - use the same imputation strategy as training data
    # Example (if you imputed missing values with mean):
    # for col in new_data.columns:
    #     if new_data[col].isnull().any():
    #         new_data[col].fillna(new_data[col].mean(), inplace=True)

    # One-hot encode categorical features (using the same categories as in training)
    new_data = pd.get_dummies(new_data, drop_first=True)

    # Align columns with training data
    missing_cols = set(X_train.columns) - set(new_data.columns)
    for col in missing_cols:
        new_data[col] = 0
    new_data = new_data[X_train.columns] # Ensure the order of columns is the same

    return new_data


# 3. Create a prediction function
def predict_footfall(new_data_point):
    # Preprocess the new data point
    preprocessed_data = preprocess_new_data(pd.DataFrame([new_data_point]))
    # Load the trained model
    loaded_model = joblib.load(model_filename)
    # Make the prediction
    prediction = loaded_model.predict(preprocessed_data)
    return prediction[0]  # Return the prediction (single value)

# Example usage:
# Sample new data (replace with actual new data)
new_data = {
    # Your new data point. Make sure to include all necessary features
    # that were used in your training data, but not the target variable
    # Example:
    'Holiday': 0,
    'Weekday': 'Monday', # If Weekday was encoded in your training set
    # other features ...
}


prediction = predict_footfall(new_data)
print(f"Predicted footfall: {prediction}")